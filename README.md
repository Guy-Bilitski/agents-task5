# LLM Context Window Experiments

A comprehensive suite of 4 experiments investigating how Large Language Models (LLMs) handle varying context conditions, information retrieval, and memory management strategies.

## ğŸ“‹ Overview

This repository contains systematic experiments designed to explore critical aspects of LLM behavior:

1. **Lost in the Middle** - Positional bias in context retrieval
2. **Context Window Size Impact** - Performance scaling with context length
3. **RAG vs Full Context** - Retrieval-augmented generation efficiency
4. **Context Management Strategies** - Memory strategies for multi-step tasks

## ğŸš€ Quick Start

### Prerequisites

```bash
# Start Ollama (for Task 1 - real LLM testing)
docker run -d --name ollama -p 11434:11434 ollama/ollama:latest
docker exec ollama ollama pull llama3.2:1b

# Install dependencies
pip install -r requirements.txt
```

### Running Experiments

Each experiment can be run independently:

```bash
# Task 1: Lost in the Middle
cd task1_experiment && python3 src/run_experiment.py

# Task 2: Context Window Size Impact
cd task2_experiment && python3 src/run_experiment.py

# Task 3: RAG vs Full Context
cd task3_experiment && python3 src/run_experiment.py

# Task 4: Context Management Strategies
cd task4_experiment && python3 src/run_experiment.py
```

## ğŸ“‚ Project Structure

```
agents-task5/
â”œâ”€â”€ common/                      # Shared utilities across all experiments
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ utils.py                # Logging, config, I/O utilities
â”‚   â”œâ”€â”€ llm.py                  # Mock LLM simulators
â”‚   â””â”€â”€ data.py                 # Text generation utilities
â”‚
â”œâ”€â”€ task1_experiment/           # Lost in the Middle
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ experiment.yaml     # Experiment configuration
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ run_experiment.py   # Main experiment script
â”‚   â”œâ”€â”€ logs/                   # Generated logs
â”‚   â”œâ”€â”€ results/                # JSON results
â”‚   â””â”€â”€ README.md               # Experiment documentation
â”‚
â”œâ”€â”€ task2_experiment/           # Context Window Size Impact
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ results/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ task3_experiment/           # RAG vs Full Context
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ data/              # Document generation
â”‚   â”‚   â”œâ”€â”€ rag/               # FAISS-based retrieval
â”‚   â”‚   â”œâ”€â”€ models/            # LLM simulation
â”‚   â”‚   â””â”€â”€ evaluation/        # Metrics calculation
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ results/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ task4_experiment/           # Context Management Strategies
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ strategies.py      # SELECT, COMPRESS, WRITE strategies
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ results/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ requirements.txt            # Project dependencies
â””â”€â”€ README.md                   # This file
```

## ğŸ”¬ Experiments

### Task 1: Lost in the Middle

**Research Question:** Do LLMs retrieve facts better from the start/end versus the middle of context?

**Hypothesis:** Facts at edges (start/end) are retrieved more accurately than those in the middle.

**Method:** Insert a critical fact at different positions (0%, 50%, 100%) in documents and measure retrieval accuracy.

**Expected Outcome:** Higher accuracy for start/end positions, demonstrating positional bias.

[ğŸ“– Full Documentation](task1_experiment/README.md)

---

### Task 2: Context Window Size Impact

**Research Question:** How do accuracy and latency change as context size increases from 2 to 50 documents?

**Hypothesis:** Latency increases linearly (or super-linearly) while accuracy degrades with larger contexts.

**Method:** Test model performance with varying document counts (2, 5, 10, 20, 50).

**Expected Outcome:** Demonstrate the trade-off between context size and performance.

[ğŸ“– Full Documentation](task2_experiment/README.md)

---

### Task 3: RAG vs Full Context

**Research Question:** Is Retrieval-Augmented Generation (RAG) more efficient than full context processing?

**Hypothesis:** RAG reduces latency and improves accuracy by filtering irrelevant information.

**Method:** Compare full-context vs FAISS-based RAG retrieval on multi-domain documents.

**Technology:** FAISS, Sentence Transformers, multilingual embeddings.

**Expected Outcome:** RAG demonstrates better efficiency and accuracy by reducing noise.

[ğŸ“– Full Documentation](task3_experiment/README.md)

---

### Task 4: Context Management Strategies

**Research Question:** Which memory management strategy works best for multi-step agentic tasks?

**Strategies Tested:**
- **SELECT (RAG):** Retrieve only relevant historical context
- **COMPRESS (Summarization):** Periodically summarize history to reduce tokens
- **WRITE (Scratchpad):** Maintain structured external memory

**Expected Outcome:**
- SELECT: âœ… PASS (retrieves key information)
- COMPRESS: âŒ FAIL (loses specific details in summary)
- WRITE: âœ… PASS (preserves structured facts)

[ğŸ“– Full Documentation](task4_experiment/README.md)

## ğŸ“Š Output Format

Each experiment generates:

- **Logs:** `logs/<experiment_name>.log` - Detailed execution logs
- **Results:** `results/results_<timestamp>.json` - Structured experiment data including:
  - Configuration used
  - Raw measurements
  - Statistical summaries
  - Hypothesis validation

## ğŸ› ï¸ Configuration

Each experiment has a `config/experiment.yaml` file that can be customized:

- **Experiment metadata:** Name, description, seed for reproducibility
- **Model settings:** URL, name, temperature, token limits
- **Dataset parameters:** Document sizes, test cases, queries
- **Logging options:** Level, console/file output
- **Output directories:** Results and log paths

## ğŸ§ª Testing Methodology

All experiments follow a consistent methodology:

1. **Reproducibility:** Fixed random seeds for consistent results
2. **Logging:** Comprehensive logging at INFO level
3. **Structured Output:** JSON results with timestamps
4. **Hypothesis-Driven:** Clear expected outcomes
5. **Modularity:** Shared common utilities, experiment-specific logic

## ğŸ“¦ Dependencies

- **Core:** `pyyaml`, `requests`
- **Task 3 specific:** `faiss-cpu`, `sentence-transformers`, `numpy`, `torch`

See `requirements.txt` for full details.

## ğŸ“ˆ Expected Results Summary

| Task | Hypothesis | Expected Outcome |
|------|------------|------------------|
| 1 | Edges > Middle | Start/End accuracy > Middle accuracy |
| 2 | Size â†‘ â†’ Performance â†“ | Latency â†‘, Accuracy â†“ with more docs |
| 3 | RAG > Full Context | RAG: Lower latency, higher accuracy |
| 4 | Structured memory best | SELECT âœ…, COMPRESS âŒ, WRITE âœ… |

## ğŸ¯ Key Insights

- **Positional Bias:** LLMs exhibit "Lost in the Middle" phenomenon
- **Scalability Trade-offs:** Larger contexts incur latency and accuracy costs
- **RAG Efficiency:** Retrieval-based methods outperform brute-force approaches
- **Memory Strategies:** Structured external memory preserves critical information better than compression

## ğŸ“ License

This project is created for educational and research purposes.

## ğŸ‘¥ Author

Created as part of LLM context window research experiments.
